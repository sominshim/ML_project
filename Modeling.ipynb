{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGK9nCkhJ6FEqlKwN+IlHE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sominshim/Predicting_Personality_through_Text/blob/somin/Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cucm7taXSKiR",
        "outputId": "25b366d3-5913-4ea9-8e03-76ffedc6823d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# !pip install tensorflow-hub\n",
        "# !pip install tfds-nightly\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(\"버전: \", tf.__version__)\n",
        "print(\"즉시 실행 모드: \", tf.executing_eagerly())\n",
        "print(\"허브 버전: \", hub.__version__)\n",
        "print(\"GPU\", \"사용 가능\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"사용 불가능\")\n",
        "\n",
        "#io와 pandas 모듈 import\n",
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "버전:  2.3.0\n",
            "즉시 실행 모드:  True\n",
            "허브 버전:  0.10.0\n",
            "GPU 사용 가능\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_hnUVG-dsqG"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras import initializers\n",
        "from tensorflow.python.keras import regularizers\n",
        "\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.layers import Dropout\n",
        "from tensorflow.python.keras.layers import Embedding\n",
        "from tensorflow.python.keras.layers import SeparableConv1D\n",
        "from tensorflow.python.keras.layers import MaxPooling1D\n",
        "from tensorflow.python.keras.layers import GlobalAveragePooling1D"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztOknNLtSR_Y",
        "outputId": "6d8ac70d-42ec-416c-ab7a-2f6ab6085a09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#드라이브에 접근할 수 있도록 아래 코드 입력\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lard_QtBSVzW"
      },
      "source": [
        "#불러올 파일의 경로를 filename 변수에 저장\n",
        "filename = '/content/drive/My Drive/mbti_1.csv'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnSasudoSWpy",
        "outputId": "87c3b3da-68d3-44a8-bab3-e45770c1ef08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#pandas read_csv로 불러오기\n",
        "data = pd.read_csv(filename)\n",
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>posts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>INFJ</td>\n",
              "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENTP</td>\n",
              "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>INTP</td>\n",
              "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>INTJ</td>\n",
              "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENTJ</td>\n",
              "      <td>'You're fired.|||That's another silly misconce...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   type                                              posts\n",
              "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
              "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
              "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
              "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
              "4  ENTJ  'You're fired.|||That's another silly misconce..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb0vareyUpuh",
        "outputId": "ec016ccc-6dae-4ed3-9ebe-7a110798c7ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Split mbti personality into 4 letters and binarize\n",
        "titles = [\"Extraversion (E) - Introversion (I)\",\n",
        "          \"Sensation (S) - INtuition (N)\",\n",
        "          \"Thinking (T) - Feeling (F)\",\n",
        "          \"Judgement (J) - Perception (P)\"\n",
        "         ] \n",
        "b_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\n",
        "b_Pers_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n",
        "\n",
        "\n",
        "#transform mbti to binary vector\n",
        "def translate_personality(personality):\n",
        "    return [b_Pers[l] for l in personality]\n",
        "\n",
        "#transform binary vector to mbti personality\n",
        "def translate_back(personality):\n",
        "    s = \"\"\n",
        "    for i, l in enumerate(personality):\n",
        "        s += b_Pers_list[i][l]\n",
        "    return s\n",
        "\n",
        "list_personality_bin = np.array([translate_personality(p) for p in data.type])\n",
        "print(\"Binarize MBTI list: \\n%s\" % list_personality_bin)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Binarize MBTI list: \n",
            "[[0 0 0 0]\n",
            " [1 0 1 1]\n",
            " [0 0 1 1]\n",
            " ...\n",
            " [0 0 1 1]\n",
            " [0 0 0 1]\n",
            " [0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiNzG8I8UrqR"
      },
      "source": [
        "data['I-E'] = list_personality_bin[:,0]\n",
        "data['N-S'] = list_personality_bin[:,1]\n",
        "data['F-T'] = list_personality_bin[:,2]\n",
        "data['J-P'] = list_personality_bin[:,3]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4PQPwXrUuTK",
        "outputId": "f19a9ee3-2827-438a-9431-ac15d8b70087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>posts</th>\n",
              "      <th>I-E</th>\n",
              "      <th>N-S</th>\n",
              "      <th>F-T</th>\n",
              "      <th>J-P</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>INFJ</td>\n",
              "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENTP</td>\n",
              "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>INTP</td>\n",
              "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>INTJ</td>\n",
              "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENTJ</td>\n",
              "      <td>'You're fired.|||That's another silly misconce...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   type                                              posts  I-E  N-S  F-T  J-P\n",
              "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...    0    0    0    0\n",
              "1  ENTP  'I'm finding the lack of me in these posts ver...    1    0    1    1\n",
              "2  INTP  'Good one  _____   https://www.youtube.com/wat...    0    0    1    1\n",
              "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...    0    0    1    0\n",
              "4  ENTJ  'You're fired.|||That's another silly misconce...    1    0    1    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SX00AbS6TTB",
        "outputId": "9c3da232-58c7-4302-9972-0d166240188b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data.columns"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['type', 'posts', 'I-E', 'N-S', 'F-T', 'J-P', 'clean_post'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Jqx9MHPJEVC",
        "outputId": "0d3fb465-8664-42f8-ad1c-863a22f3ba8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data['posts'][:5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
              "1    'I'm finding the lack of me in these posts ver...\n",
              "2    'Good one  _____   https://www.youtube.com/wat...\n",
              "3    'Dear INTP,   I enjoyed our conversation the o...\n",
              "4    'You're fired.|||That's another silly misconce...\n",
              "Name: posts, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDurCxSWNk7A"
      },
      "source": [
        "## 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0t3ErosOtOk"
      },
      "source": [
        "### 4. Replace Contractions\n",
        "This techniques replaces contractions to their equivalents.\n",
        "\n",
        "Example: What's the scariest thing that ever happened to anyone? -> What is the scariest thing that ever happened to anyone?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovgkkvPBNkmk"
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import re\n",
        "import string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCGryo5ANPMZ"
      },
      "source": [
        "contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'I\\'m', 'I am'),(r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
        "                         (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\n",
        "def replaceContraction(text):\n",
        "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
        "    for (pattern, repl) in patterns:\n",
        "        (text, count) = re.subn(pattern, repl, text)\n",
        "    return text\n",
        "\n",
        "text_replaceContractions = pd.DataFrame(columns=['TextBefore', 'TextAfter', 'Changed'])\n",
        "text_replaceContractions['TextBefore'] = data['posts'].copy()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7jIlt_qNSYA"
      },
      "source": [
        "for index, row in text_replaceContractions.iterrows():\n",
        "    row['TextAfter'] = replaceContraction(row['TextBefore'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foV233_qNulG",
        "outputId": "cd3fba5d-7461-4ad5-b944-848bda88eac7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(text_replaceContractions['TextBefore'][:5],'->', text_replaceContractions['TextAfter'][:5])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
            "1    'I'm finding the lack of me in these posts ver...\n",
            "2    'Good one  _____   https://www.youtube.com/wat...\n",
            "3    'Dear INTP,   I enjoyed our conversation the o...\n",
            "4    'You're fired.|||That's another silly misconce...\n",
            "Name: TextBefore, dtype: object -> 0    'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
            "1    'I am finding the lack of me in these posts ve...\n",
            "2    'Good one  _____   https://www.youtube.com/wat...\n",
            "3    'Dear INTP,   I enjoyed our conversation the o...\n",
            "4    'You are fired.|||That is another silly miscon...\n",
            "Name: TextAfter, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8bSHiUoPqxN"
      },
      "source": [
        "### 특수문자 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSpK8JSxSc9y"
      },
      "source": [
        "def cleaner(post):\n",
        "    # 소문자 변환\n",
        "    post = post.lower() \n",
        "    \n",
        "    # '|||' 제거\n",
        "    post = post.replace('|||', \"\") \n",
        "\n",
        "    # URL, 링크 등 특수문자 제거\n",
        "    post = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', '', post, flags=re.MULTILINE) \n",
        "\n",
        "    # 구두점 제거\n",
        "    post = post.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # 공백 제거\n",
        "    post = re.sub( '\\s+', ' ', post ).strip()\n",
        "\n",
        "    # MBTI 제거\n",
        "    post = re.sub('infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj',\"\", post)\n",
        "\n",
        "    return post\n",
        "  \n",
        "posts = text_replaceContractions.TextAfter.tolist()\n",
        "posts = [cleaner(post) for post in posts]\n",
        "data['clean_post'] = posts"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yfNWVJGQNaE",
        "outputId": "2163b0ff-b9e3-4c43-8f3c-4eb1de2f82d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(text_replaceContractions['TextAfter'][:5],'\\n V\\n', data['clean_post'][:5])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
            "1    'I am finding the lack of me in these posts ve...\n",
            "2    'Good one  _____   https://www.youtube.com/wat...\n",
            "3    'Dear INTP,   I enjoyed our conversation the o...\n",
            "4    'You are fired.|||That is another silly miscon...\n",
            "Name: TextAfter, dtype: object \n",
            " V\n",
            " 0    and  moments sportscenter not top ten plays pr...\n",
            "1    i am finding the lack of me in these posts ver...\n",
            "2    good one course to which i say i know that is ...\n",
            "3    dear  i enjoyed our conversation the other day...\n",
            "4    you are firedthat is another silly misconcepti...\n",
            "Name: clean_post, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwmJfpNRO8Dg"
      },
      "source": [
        "### 8. Remove Stopwords\n",
        "Example: How I know whether a girl had done sex before sex with me? -> How I know whether girl done sex sex ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up3Hhy0iI6qw",
        "outputId": "06097dcd-3e37-4951-a591-cb833cc042c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stoplist = stopwords.words('english')\n",
        "\n",
        "def tokenize(text):\n",
        "    finalTokens = []\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    for w in tokens:\n",
        "        if (w not in stoplist):\n",
        "            finalTokens.append(w)\n",
        "    text = \" \".join(finalTokens)\n",
        "    return text"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffhGS_wlKFbv"
      },
      "source": [
        "text_removeStopwords = pd.DataFrame(columns=['TextBefore', 'TextAfter', 'Changed'])\n",
        "text_removeStopwords['TextBefore'] = data['clean_post'].copy()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTx0f-oAJIZG"
      },
      "source": [
        "for index, row in text_removeStopwords.iterrows():\n",
        "    row['TextAfter'] = tokenize(row['TextBefore'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBhseEq8JzZM",
        "outputId": "81c19a9f-ed5f-4202-d666-39bc78b2547d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(data['clean_post'][:5],'->',text_removeStopwords['TextAfter'][:5])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    and  moments sportscenter not top ten plays pr...\n",
            "1    i am finding the lack of me in these posts ver...\n",
            "2    good one course to which i say i know that is ...\n",
            "3    dear  i enjoyed our conversation the other day...\n",
            "4    you are firedthat is another silly misconcepti...\n",
            "Name: clean_post, dtype: object -> 0    moments sportscenter top ten plays prankswhat ...\n",
            "1    finding lack posts alarmingsex boring position...\n",
            "2    good one course say know blessing cursedoes ab...\n",
            "3    dear enjoyed conversation day esoteric gabbing...\n",
            "4    firedthat another silly misconception approach...\n",
            "Name: TextAfter, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve1hKK36RqTE"
      },
      "source": [
        "data['clean_post'] = text_removeStopwords['TextAfter']"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odw2tXlNRW_J"
      },
      "source": [
        "### We can correct manually most frequent mispells\n",
        "For example, here are some mistakes and their frequency\n",
        "qoura : 85 times\n",
        "mastrubation : 38 times\n",
        "demonitisation : 30 times\n",
        "…"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBBvS6GoRWep"
      },
      "source": [
        "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', 'pokémon': 'pokemon'}\n",
        "def correct_spelling(x, dic):\n",
        "    for word in dic.keys():\n",
        "        x = x.replace(word, dic[word])\n",
        "    return x"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seY8_TsoM9H9"
      },
      "source": [
        "data['clean_post'] = data['clean_post'].apply(lambda x: correct_spelling(x, mispell_dict))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WQLD361SMkU",
        "outputId": "16eeb290-ba93-49f9-e590-60ca299bc1c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data['clean_post'][:5]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    moments sportscenter top ten plays prankswhat ...\n",
              "1    finding lack posts alarmingsex boring position...\n",
              "2    good one course say know blessing cursedoes ab...\n",
              "3    dear enjoyed conversation day esoteric gabbing...\n",
              "4    firedthat another silly misconception approach...\n",
              "Name: clean_post, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfe5mrUCVy0O"
      },
      "source": [
        "## Train/ Test/ Validation data Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBqgOYpJTRmQ"
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in split.split(data, data[['type']]):\n",
        "    strat_train_set = data.loc[train_index]\n",
        "    strat_test_set = data.loc[test_index]\n",
        "\n",
        "X_train =  strat_train_set['clean_post']\n",
        "y_train = list_personality_bin[train_index]\n",
        "X_test = strat_test_set['clean_post']\n",
        "y_test = list_personality_bin[test_index]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCuBnmKmU-q2",
        "outputId": "91a0a843-3ac4-4b7d-b919-9871f3f392b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f\"train_articles {len(X_train)}\")\n",
        "print(\"train_labels\", len(y_train))\n",
        "print(\"validation_articles\", len(X_test))\n",
        "print(\"validation_labels\", len(y_test))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_articles 6940\n",
            "train_labels 6940\n",
            "validation_articles 1735\n",
            "validation_labels 1735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0vzT9ak4vhr"
      },
      "source": [
        "#\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#\n",
        "from sklearn import model_selection\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ4t8t3P-hoV",
        "outputId": "befdb981-ece9-45a4-b0a4-aa03e43fa476",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "categories = ['I-E', 'N-S', 'F-T', 'J-P']\n",
        "\n",
        "train, test = train_test_split(data, random_state=42, test_size=0.2, shuffle=True)\n",
        "X_train = train.clean_post\n",
        "X_test = test.clean_post\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6940,)\n",
            "(1735,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33g4Y2kl_FRH",
        "outputId": "49b8054d-c83a-4814-d03e-d3e45a895f6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import re\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "import seaborn as sns\n",
        "\n",
        "# Define a pipeline combining a text feature extractor with multi lable classifier\n",
        "NB_pipeline = Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
        "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
        "                    fit_prior=True, class_prior=None))),\n",
        "            ])\n",
        "for category in categories:\n",
        "    print('... Processing {}'.format(category))\n",
        "    # train the model using X_dtm & y\n",
        "    NB_pipeline.fit(X_train, train[category])\n",
        "    # compute the testing accuracy\n",
        "    prediction = NB_pipeline.predict(X_test)\n",
        "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "... Processing I-E\n",
            "Test accuracy is 0.7798270893371758\n",
            "... Processing N-S\n",
            "Test accuracy is 0.8582132564841498\n",
            "... Processing F-T\n",
            "Test accuracy is 0.5440922190201729\n",
            "... Processing J-P\n",
            "Test accuracy is 0.6144092219020173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAqvLzg1_lop",
        "outputId": "328a77d2-549c-45c0-9369-cd75d6abcf95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "SVC_pipeline = Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
        "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
        "            ])\n",
        "for category in categories:\n",
        "    print('... Processing {}'.format(category))\n",
        "    # train the model using X_dtm & y\n",
        "    SVC_pipeline.fit(X_train, train[category])\n",
        "    # compute the testing accuracy\n",
        "    prediction = SVC_pipeline.predict(X_test)\n",
        "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "... Processing I-E\n",
            "Test accuracy is 0.7959654178674351\n",
            "... Processing N-S\n",
            "Test accuracy is 0.8622478386167147\n",
            "... Processing F-T\n",
            "Test accuracy is 0.7878962536023055\n",
            "... Processing J-P\n",
            "Test accuracy is 0.6824207492795389\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64sm2qYf_wmt",
        "outputId": "57f08bad-9e12-49aa-ffc5-e14dc504f644",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "LogReg_pipeline = Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
        "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),\n",
        "            ])\n",
        "for category in categories:\n",
        "    print('... Processing {}'.format(category))\n",
        "    # train the model using X_dtm & y\n",
        "    LogReg_pipeline.fit(X_train, train[category])\n",
        "    # compute the testing accuracy\n",
        "    prediction = LogReg_pipeline.predict(X_test)\n",
        "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "... Processing I-E\n",
            "Test accuracy is 0.7855907780979827\n",
            "... Processing N-S\n",
            "Test accuracy is 0.8582132564841498\n",
            "... Processing F-T\n",
            "Test accuracy is 0.7792507204610951\n",
            "... Processing J-P\n",
            "Test accuracy is 0.6858789625360231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWhhIiIyBaoh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}