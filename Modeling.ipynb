{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPl8FNEnQc/fqP/2TN/fjSW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sominshim/Predicting_Personality_through_Text/blob/main/Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGiyWu1dJURX",
        "outputId": "8ec5be0b-7596-48fd-c310-86b160b9eae1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "# !pip install tensorflow-hub\n",
        "# !pip install tfds-nightly\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(\"버전: \", tf.__version__)\n",
        "print(\"즉시 실행 모드: \", tf.executing_eagerly())\n",
        "print(\"허브 버전: \", hub.__version__)\n",
        "print(\"GPU\", \"사용 가능\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"사용 불가능\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "버전:  2.3.0\n",
            "즉시 실행 모드:  True\n",
            "허브 버전:  0.10.0\n",
            "GPU 사용 가능\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cucm7taXSKiR"
      },
      "source": [
        "# import module\n",
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-JmVkorJ26t"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_hnUVG-dsqG"
      },
      "source": [
        "# import math\n",
        "# from tensorflow.python.keras import models\n",
        "# from tensorflow.python.keras import initializers\n",
        "# from tensorflow.python.keras import regularizers\n",
        "\n",
        "# from tensorflow.python.keras.layers import Dense\n",
        "# from tensorflow.python.keras.layers import Dropout\n",
        "# from tensorflow.python.keras.layers import Embedding\n",
        "# from tensorflow.python.keras.layers import SeparableConv1D\n",
        "# from tensorflow.python.keras.layers import MaxPooling1D\n",
        "# from tensorflow.python.keras.layers import GlobalAveragePooling1D"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztOknNLtSR_Y",
        "outputId": "12e1caf2-0ee7-4502-f739-36e4e29d4a64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#드라이브에 접근할 수 있도록 아래 코드 입력\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lard_QtBSVzW"
      },
      "source": [
        "#불러올 파일의 경로를 filename 변수에 저장\n",
        "filename = '/content/drive/My Drive/mbti_1.csv'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnSasudoSWpy",
        "outputId": "742187a1-a561-4162-bf40-c11c8f8ab401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#pandas read_csv로 불러오기\n",
        "data = pd.read_csv(filename)\n",
        "data.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>posts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>INFJ</td>\n",
              "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENTP</td>\n",
              "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>INTP</td>\n",
              "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>INTJ</td>\n",
              "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENTJ</td>\n",
              "      <td>'You're fired.|||That's another silly misconce...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   type                                              posts\n",
              "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
              "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
              "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
              "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
              "4  ENTJ  'You're fired.|||That's another silly misconce..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb0vareyUpuh",
        "outputId": "84d2463c-6773-42fc-e8c2-9865f153534d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Split mbti personality into 4 letters and binarize\n",
        "titles = [\"Extraversion (E) - Introversion (I)\",\n",
        "          \"Sensation (S) - INtuition (N)\",\n",
        "          \"Thinking (T) - Feeling (F)\",\n",
        "          \"Judgement (J) - Perception (P)\"\n",
        "         ] \n",
        "b_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\n",
        "b_Pers_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n",
        "\n",
        "\n",
        "#transform mbti to binary vector\n",
        "def translate_personality(personality):\n",
        "    return [b_Pers[l] for l in personality]\n",
        "\n",
        "#transform binary vector to mbti personality\n",
        "def translate_back(personality):\n",
        "    s = \"\"\n",
        "    for i, l in enumerate(personality):\n",
        "        s += b_Pers_list[i][l]\n",
        "    return s\n",
        "\n",
        "list_personality_bin = np.array([translate_personality(p) for p in data.type])\n",
        "print(\"Binarize MBTI list: \\n%s\" % list_personality_bin)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Binarize MBTI list: \n",
            "[[0 0 0 0]\n",
            " [1 0 1 1]\n",
            " [0 0 1 1]\n",
            " ...\n",
            " [0 0 1 1]\n",
            " [0 0 0 1]\n",
            " [0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiNzG8I8UrqR"
      },
      "source": [
        "data['I-E'] = list_personality_bin[:,0]\n",
        "data['N-S'] = list_personality_bin[:,1]\n",
        "data['F-T'] = list_personality_bin[:,2]\n",
        "data['J-P'] = list_personality_bin[:,3]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4PQPwXrUuTK",
        "outputId": "4694ead5-dcdc-4bba-fa68-66854cc2972c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>posts</th>\n",
              "      <th>I-E</th>\n",
              "      <th>N-S</th>\n",
              "      <th>F-T</th>\n",
              "      <th>J-P</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>INFJ</td>\n",
              "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENTP</td>\n",
              "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>INTP</td>\n",
              "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>INTJ</td>\n",
              "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENTJ</td>\n",
              "      <td>'You're fired.|||That's another silly misconce...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   type                                              posts  I-E  N-S  F-T  J-P\n",
              "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...    0    0    0    0\n",
              "1  ENTP  'I'm finding the lack of me in these posts ver...    1    0    1    1\n",
              "2  INTP  'Good one  _____   https://www.youtube.com/wat...    0    0    1    1\n",
              "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...    0    0    1    0\n",
              "4  ENTJ  'You're fired.|||That's another silly misconce...    1    0    1    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Jqx9MHPJEVC",
        "outputId": "4e2bf8fd-6e52-44ab-ddb8-b897e6b1d80e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data['posts'][:5]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
              "1    'I'm finding the lack of me in these posts ver...\n",
              "2    'Good one  _____   https://www.youtube.com/wat...\n",
              "3    'Dear INTP,   I enjoyed our conversation the o...\n",
              "4    'You're fired.|||That's another silly misconce...\n",
              "Name: posts, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDurCxSWNk7A"
      },
      "source": [
        "## 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0t3ErosOtOk"
      },
      "source": [
        "### 4. Replace Contractions\n",
        "This techniques replaces contractions to their equivalents.\n",
        "\n",
        "Example: What's the scariest thing that ever happened to anyone? -> What is the scariest thing that ever happened to anyone?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovgkkvPBNkmk"
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import re\n",
        "import string"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCGryo5ANPMZ"
      },
      "source": [
        "contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'I\\'m', 'I am'),(r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
        "                         (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\n",
        "def replace(text):\n",
        "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
        "    for (pattern, repl) in patterns:\n",
        "        (text, count) = re.subn(pattern, repl, text)\n",
        "    return text\n",
        "\n",
        "text_replace = pd.DataFrame(columns=['TextBefore', 'TextAfter'])\n",
        "text_replace['TextBefore'] = data['posts'].copy()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7jIlt_qNSYA"
      },
      "source": [
        "for index, row in text_replace.iterrows():\n",
        "    row['TextAfter'] = replace(row['TextBefore'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foV233_qNulG",
        "outputId": "1a0d52bf-b08a-4fa3-ab9a-32cbebf1a00c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(text_replace['TextBefore'][:5],'\\n->\\n', text_replace['TextAfter'][:5])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
            "1    'I'm finding the lack of me in these posts ver...\n",
            "2    'Good one  _____   https://www.youtube.com/wat...\n",
            "3    'Dear INTP,   I enjoyed our conversation the o...\n",
            "4    'You're fired.|||That's another silly misconce...\n",
            "Name: TextBefore, dtype: object \n",
            "->\n",
            " 0    'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
            "1    'I am finding the lack of me in these posts ve...\n",
            "2    'Good one  _____   https://www.youtube.com/wat...\n",
            "3    'Dear INTP,   I enjoyed our conversation the o...\n",
            "4    'You are fired.|||That is another silly miscon...\n",
            "Name: TextAfter, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8bSHiUoPqxN"
      },
      "source": [
        "### 특수문자 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSpK8JSxSc9y"
      },
      "source": [
        "def cleaner(post):\n",
        "    # 소문자 변환\n",
        "    post = post.lower() \n",
        "    \n",
        "    # '|||' 제거\n",
        "    post = post.replace('|||', \"\") \n",
        "\n",
        "    # URL, 링크 등 특수문자 제거\n",
        "    post = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', '', post, flags=re.MULTILINE) \n",
        "\n",
        "    # 구두점 제거\n",
        "    post = post.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # 공백 제거\n",
        "    post = re.sub( '\\s+', ' ', post ).strip()\n",
        "\n",
        "    # MBTI 제거\n",
        "    post = re.sub('infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj',\"\", post)\n",
        "\n",
        "    # 길이가 짧은 단어 제거\n",
        "    shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "    post = shortword.sub('', post)\n",
        "    \n",
        "    return post"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YAC-_VRLCqM"
      },
      "source": [
        "posts = text_replace.TextAfter.tolist()\n",
        "posts = [cleaner(post) for post in posts]\n",
        "data['clean_post'] = posts"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yfNWVJGQNaE",
        "outputId": "0eda74a8-954e-4f85-877b-4b7239e44e3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(text_replace['TextAfter'][:5],'\\n->\\n', data['clean_post'][:5])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
            "1    'I am finding the lack of me in these posts ve...\n",
            "2    'Good one  _____   https://www.youtube.com/wat...\n",
            "3    'Dear INTP,   I enjoyed our conversation the o...\n",
            "4    'You are fired.|||That is another silly miscon...\n",
            "Name: TextAfter, dtype: object \n",
            "->\n",
            " 0    and  moments sportscenter not top ten plays pr...\n",
            "1     finding the lack these posts very alarmingsex...\n",
            "2    good one course which say know that blessing a...\n",
            "3    dear enjoyed our conversation the other day es...\n",
            "4    you are firedthat another silly misconception ...\n",
            "Name: clean_post, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwmJfpNRO8Dg"
      },
      "source": [
        "### Remove Stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up3Hhy0iI6qw",
        "outputId": "b49d5626-9604-4ce0-c0f7-7b05b4e24e43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stoplist = stopwords.words('english')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "\n",
        "n=WordNetLemmatizer()\n",
        "s = PorterStemmer()\n",
        "\n",
        "def tokenize(text):\n",
        "    finalTokens = []\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    for w in tokens:\n",
        "        if (w not in stoplist):\n",
        "            finalTokens.append(w)\n",
        "    lem  = [n.lemmatize(w) for w in finalTokens]\n",
        "    stem = [s.stem(w) for w in lem]\n",
        "    text = \" \".join(stem)\n",
        "\n",
        "    return text"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffhGS_wlKFbv"
      },
      "source": [
        "text_removeStopwords = pd.DataFrame(columns=['TextBefore', 'TextAfter'])\n",
        "text_removeStopwords['TextBefore'] = data['clean_post'].copy()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTx0f-oAJIZG",
        "outputId": "5d27e039-cd55-4276-d28f-3c6f959c5d4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for index, row in text_removeStopwords.iterrows():\n",
        "    row['TextAfter'] = tokenize(row['TextBefore'])\n",
        "\n",
        "data['clean_post'] = text_removeStopwords['TextAfter']\n",
        "print(data['clean_post'][:5],'\\n->\\n',text_removeStopwords['TextAfter'][:5])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    moment sportscent top ten play prankswhat life...\n",
            "1    find lack post alarmingsex bore posit often ex...\n",
            "2    good one cours say know bless cursedo absolut ...\n",
            "3    dear enjoy convers day esoter gab natur univer...\n",
            "4    firedthat anoth silli misconcept approach logi...\n",
            "Name: clean_post, dtype: object \n",
            "->\n",
            " 0    moment sportscent top ten play prankswhat life...\n",
            "1    find lack post alarmingsex bore posit often ex...\n",
            "2    good one cours say know bless cursedo absolut ...\n",
            "3    dear enjoy convers day esoter gab natur univer...\n",
            "4    firedthat anoth silli misconcept approach logi...\n",
            "Name: TextAfter, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odw2tXlNRW_J"
      },
      "source": [
        "### most frequent mispells -> correct manually \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBBvS6GoRWep"
      },
      "source": [
        "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', 'pokémon': 'pokemon'}\n",
        "def correct_spelling(x, dic):\n",
        "    for word in dic.keys():\n",
        "        x = x.replace(word, dic[word])\n",
        "    return x"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seY8_TsoM9H9",
        "outputId": "a90b5f6f-0e0c-4e22-f176-0937758a4fc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data['clean_post'] = data['clean_post'].apply(lambda x: correct_spelling(x, mispell_dict))\n",
        "data['clean_post'][:5]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    moments sportscenter top ten plays prankswhat ...\n",
              "1    finding lack posts alarmingsex boring position...\n",
              "2    good one course say know blessing cursedoes ab...\n",
              "3    dear enjoyed conversation day esoteric gabbing...\n",
              "4    firedthat another silly misconception approach...\n",
              "Name: clean_post, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p2dx56OVdwl"
      },
      "source": [
        "sentences = data['clean_post']\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfVvGWseVll8",
        "outputId": "dfc7544d-67a4-48aa-8bc6-fb4170503d73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print(encoded)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeGYWz6DVngX",
        "outputId": "ee2f8795-6148-4e7e-d3c7-38d1b753ab7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "max_len = max(len(item) for item in encoded)\n",
        "print(max_len)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l-_Pfv1VqcS",
        "outputId": "17609d83-8234-4d91-cc67-950805e1ccac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for item in encoded: # 각 문장에 대해서\n",
        "    while len(item) < max_len:   # max_len보다 작으면\n",
        "        item.append(0)\n",
        "\n",
        "padded_np = np.array(encoded)\n",
        "padded_np"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  267, 67370,   543, ...,     0,     0,     0],\n",
              "       [   35,   394,    48, ...,     0,     0,     0],\n",
              "       [   22,     7,   249, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [   65,    76,    10, ...,     0,     0,     0],\n",
              "       [  774,    53,    50, ...,     0,     0,     0],\n",
              "       [   86,   104,  3811, ...,     0,     0,     0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfe5mrUCVy0O"
      },
      "source": [
        "## Train/ Test/ Validation data Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBqgOYpJTRmQ"
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in split.split(data, data[['type']]):\n",
        "    strat_train_set = data.loc[train_index]\n",
        "    strat_test_set = data.loc[test_index]\n",
        "\n",
        "X_train =  strat_train_set['clean_post']\n",
        "y_train = list_personality_bin[train_index]\n",
        "X_test = strat_test_set['clean_post']\n",
        "y_test = list_personality_bin[test_index]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCuBnmKmU-q2",
        "outputId": "dc5537cf-a8bf-49d2-e203-c493ffe71868",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f\"train_articles {len(X_train)}\")\n",
        "print(\"train_labels\", len(y_train))\n",
        "print(\"validation_articles\", len(X_test))\n",
        "print(\"validation_labels\", len(y_test))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_articles 6940\n",
            "train_labels 6940\n",
            "validation_articles 1735\n",
            "validation_labels 1735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ4t8t3P-hoV",
        "outputId": "859d2a46-13da-4cb5-e644-0ca1c5cd7e15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "categories = ['I-E', 'N-S', 'F-T', 'J-P']\n",
        "\n",
        "train, test = train_test_split(data, random_state=42, test_size=0.2, shuffle=True)\n",
        "X_train = train.clean_post\n",
        "X_test = test.clean_post\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6940,)\n",
            "(1735,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33g4Y2kl_FRH",
        "outputId": "315e9568-b533-4910-f92b-0e7886ef2488",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "NB_pipeline = Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(max_features=200)),\n",
        "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
        "                    fit_prior=True, class_prior=None))),\n",
        "            ])\n",
        "for category in categories:\n",
        "    print('... Processing {}'.format(category))\n",
        "    NB_pipeline.fit(X_train, train[category])\n",
        "    prediction = NB_pipeline.predict(X_test)\n",
        "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "... Processing I-E\n",
            "Test accuracy is 0.7798270893371758\n",
            "... Processing N-S\n",
            "Test accuracy is 0.8582132564841498\n",
            "... Processing F-T\n",
            "Test accuracy is 0.6276657060518732\n",
            "... Processing J-P\n",
            "Test accuracy is 0.6144092219020173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAqvLzg1_lop",
        "outputId": "f756f83b-16da-414a-d62a-ce621cdc0398",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "SVC_pipeline = Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(max_features=200)),\n",
        "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
        "            ])\n",
        "for category in categories:\n",
        "    print('... Processing {}'.format(category))\n",
        "    SVC_pipeline.fit(X_train, train[category])\n",
        "    prediction = SVC_pipeline.predict(X_test)\n",
        "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "... Processing I-E\n",
            "Test accuracy is 0.7792507204610951\n",
            "... Processing N-S\n",
            "Test accuracy is 0.8576368876080691\n",
            "... Processing F-T\n",
            "Test accuracy is 0.7187319884726224\n",
            "... Processing J-P\n",
            "Test accuracy is 0.6213256484149856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64sm2qYf_wmt",
        "outputId": "c646c36c-f044-4cc9-f818-64eb117329a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "LogReg_pipeline = Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(max_features=200)),\n",
        "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),\n",
        "            ])\n",
        "for category in categories:\n",
        "    print('... Processing {}'.format(category))\n",
        "    LogReg_pipeline.fit(X_train, train[category])\n",
        "    prediction = LogReg_pipeline.predict(X_test)\n",
        "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "... Processing I-E\n",
            "Test accuracy is 0.7780979827089337\n",
            "... Processing N-S\n",
            "Test accuracy is 0.8570605187319885\n",
            "... Processing F-T\n",
            "Test accuracy is 0.7170028818443804\n",
            "... Processing J-P\n",
            "Test accuracy is 0.624207492795389\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWhhIiIyBaoh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}